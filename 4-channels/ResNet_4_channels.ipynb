{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet_4_channels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJgLTnmvRiwQ"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.test.is_gpu_available()\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g87--n2AtyO_"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykea2YRJAQIa"
      },
      "source": [
        "# Creating Image Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us_GbDYbAjGr"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssoE7TbxAjVk",
        "outputId": "d70964f1-bb7c-4e0e-b26b-a311dadee989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qebASKsngQEr"
      },
      "source": [
        "sz = (128, 128)\n",
        "BATCH_SIZE = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ9HPgoNk9lm"
      },
      "source": [
        "# note that for the directories, keras data generators expect a url that points to the folder\n",
        "# that is located at the second level above the images/masks\n",
        "# folder --> directory fed to keras generator\n",
        "#### folder --> sub-directory, usually these stands to the number of clases for the classification model, since it's a segmentation task, it's only one folder\n",
        "####### images or masks\n",
        "\n",
        "# directories of the sugarbeet dataset\n",
        "rgb_images_dir = '/home/path/to/images'\n",
        "annotations_color_dir = '/home/path/to/masks'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1LRYQreLEWK",
        "outputId": "e581da2d-68e0-4e44-fc97-fb31f5d5d2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# we create two instances with the same arguments\n",
        "data_gen_args = dict(                    \n",
        "                     horizontal_flip=True,\n",
        "                     vertical_flip=True,\n",
        "                     rescale=1./255,\n",
        "                     validation_split=0.1\n",
        "                     )\n",
        "\n",
        "image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "#Splitting training and validation without applying same transformations to both datasets:\n",
        "#https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in\n",
        "#https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
        "\n",
        "# Provide the same seed and keyword arguments to the fit and flow methods\n",
        "seed = 1\n",
        "train_image_generator = image_datagen.flow_from_directory(\n",
        "    rgb_images_dir,\n",
        "    target_size = sz,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "train_mask_generator = mask_datagen.flow_from_directory(\n",
        "    annotations_color_dir,\n",
        "    target_size = sz,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "val_image_generator = image_datagen.flow_from_directory(\n",
        "    rgb_images_dir,\n",
        "    target_size = sz,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=True,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "val_mask_generator = mask_datagen.flow_from_directory(\n",
        "    annotations_color_dir,\n",
        "    target_size = sz,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=True,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        " #about resetting test generators\n",
        "#https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4224 images belonging to 1 classes.\n",
            "Found 4224 images belonging to 1 classes.\n",
            "Found 469 images belonging to 1 classes.\n",
            "Found 469 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgzTtORV9l_9"
      },
      "source": [
        "def normalize(images):\n",
        "  normalized_images = []\n",
        "  for i in range(len(images)):\n",
        "    image = images[i]\n",
        "    max_val = np.ceil(np.max(image))\n",
        "    image = image/max_val\n",
        "    normalized_images.append(image)\n",
        "  return normalized_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NweNGGLRA61m"
      },
      "source": [
        "#If this doesn't work, applying mask changes\n",
        "#https://github.com/keras-team/keras-preprocessing/issues/125\n",
        "def prepare_mask(masks):\n",
        "  new_masks = [] \n",
        "  for i in range (np.shape(masks)[0]):\n",
        "    mask_datapoint = masks[i]\n",
        "    r = mask_datapoint[:,:,0]\n",
        "    g = mask_datapoint[:,:,1]\n",
        "    b = mask_datapoint[:,:,2]\n",
        "    r[r>(150/255)]=3\n",
        "    g[g>(150/255)]=1\n",
        "    b[b>(150/255)]=2\n",
        "    x_centroid=[]\n",
        "    y_centroid=[]\n",
        "    for x in range(b.shape[0]):\n",
        "      for y in range(b.shape[1]):\n",
        "        if b[x,y]==2:\n",
        "          x_centroid.append(x)\n",
        "          y_centroid.append(y)\n",
        "    \n",
        "    for i in range(len(x_centroid)):\n",
        "      x=x_centroid[i]\n",
        "      y=y_centroid[i]\n",
        "      augm=1\n",
        "      x_augm_plus=x+augm\n",
        "      x_augm_min=x-augm\n",
        "      y_augm_plus=y+augm\n",
        "      y_augm_min=y-augm\n",
        "      \n",
        "      if(x_augm_plus<b.shape[0]):\n",
        "        b[x_augm_plus,y]=2\n",
        "      if(x_augm_min>0):\n",
        "        b[x_augm_min,y]=2\n",
        "      if(y_augm_plus<b.shape[1]):\n",
        "        b[x,y_augm_plus]=2\n",
        "      if(y_augm_min>0):\n",
        "        b[x,y_augm_min]=2\n",
        "      \n",
        "      if(x_augm_plus>0 and x_augm_plus<b.shape[0] and y_augm_plus>0 and y_augm_plus<b.shape[1]):\n",
        "        b[x_augm_plus,y_augm_plus]=2\n",
        "      if(x_augm_plus>0 and x_augm_plus<b.shape[0] and y_augm_min>0 and y_augm_min<b.shape[1]):\n",
        "        b[x_augm_plus,y_augm_min]=2\n",
        "      if(x_augm_min>0 and x_augm_min<b.shape[0] and y_augm_plus>0 and y_augm_plus<b.shape[1]):\n",
        "        b[x_augm_min,y_augm_plus]=2\n",
        "      if(x_augm_min>0 and x_augm_min<b.shape[0] and y_augm_min>0 and y_augm_min<b.shape[1]):\n",
        "        b[x_augm_min,y_augm_min]=2\n",
        "\n",
        "    merged = np.maximum.reduce([r,g,b])\n",
        "    new_masks.append(merged)\n",
        "  new_masks = np.expand_dims(new_masks,3)\n",
        "  return new_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ6ppr4W-PMC"
      },
      "source": [
        "def my_image_mask_generator(image_generator, mask_generator):\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    for (imgs, msks) in train_generator:\n",
        "        msks_total = prepare_mask(msks)\n",
        "        yield (imgs, msks_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdgYj324AqEk"
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask', 'Fourth image']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b949OT11LoQe"
      },
      "source": [
        " #for joining generators\n",
        " #https://stackoverflow.com/questions/3211041/how-to-join-two-generators-in-python\n",
        " #https://stackoverflow.com/questions/49404993/keras-how-to-use-fit-generator-with-multiple-inputs/49405175#comment89680557_49405175\n",
        "my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n",
        "my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZlZvRkitdv9"
      },
      "source": [
        "x,y = next(my_train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpyunIFvXfeo",
        "scrolled": true
      },
      "source": [
        "sample_image = x[0]\n",
        "sample_mask = y[0]\n",
        "display([sample_image,sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMOEAR63uxCx"
      },
      "source": [
        "z,w = next(my_val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "# Define the model\n",
        "The model being used here is a modified U-Net. A pretrained model (MobileNetV2) is used as the encoder. The decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6iB4iMvMkX9"
      },
      "source": [
        "OUTPUT_CHANNELS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCeLH0ctjq7"
      },
      "source": [
        "base_model = tf.keras.applications.ResNet101V2(input_shape=[128, 128, 3], include_top=False,weights=\"imagenet\")\n",
        "layer_names = [\n",
        "    'conv1_conv', #64x64\n",
        "    'conv2_block3_1_relu',   # 32x32\n",
        "    'conv3_block4_1_relu',   # 16x16\n",
        "    'conv4_block23_1_relu',  # 8x8\n",
        "    'conv5_block3_2_relu',      # 4x4\n",
        "]\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jzrmK-ok8Hp"
      },
      "source": [
        "tf.keras.utils.plot_model(down_stack, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "source": [
        "up_stack_crop = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "   pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "source": [
        "def resnet_model(output_channels, inputs):\n",
        "  #x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  print('skips', skips)\n",
        "  last_downsampled_layer = skips[-1]\n",
        "  x = last_downsampled_layer\n",
        "  skips = list(reversed(skips[:-1]))\n",
        "\n",
        "  ##################################################\n",
        "  ############# Crop/Weed branch ###################\n",
        "  ##################################################\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack_crop, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "  x = last(x)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU451nBUclqM"
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "model = resnet_model(OUTPUT_CHANNELS, inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mUtS6NUcgzG"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1_Al3GLka8H"
      },
      "source": [
        "model.load_weights(\"/home/path/to/weights/epoch\"+str(250)+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko6N3Xqf6Zn_"
      },
      "source": [
        "# Define the IoU metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBRGp8i16fBa"
      },
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "  y_pred = tf.keras.backend.cast(create_mask(y_pred), 'float32')\n",
        "  inter = tf.math.count_nonzero(tf.logical_and(tf.not_equal(y_true, 0), tf.equal(y_true,y_pred)))\n",
        "  union = tf.math.count_nonzero(tf.add(y_true, y_pred))\n",
        "  my_iou = tf.cast(inter/union, 'float32')\n",
        "  return my_iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "# Compile the model\n",
        "The network is trying to assign each pixel a label. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zL_5gnVYMiw"
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\",mean_iou]\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-zkXB11CBCG"
      },
      "source": [
        "# Viewing the model performance before training the trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vJXPz2bUAKg"
      },
      "source": [
        "threshold = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    pred_x, pred_y = next(dataset)\n",
        "    for i in range (num):\n",
        "      image, mask = pred_x[i], pred_y[i]\n",
        "      pred_mask = model.predict(image[tf.newaxis, ...])\n",
        "      my_iou_crop =  mean_iou(mask, pred_mask)\n",
        "      print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n",
        "\n",
        "      pred_mask = create_mask(pred_mask)\n",
        "      print(np.unique(pred_mask), pred_mask.shape)\n",
        "      \n",
        "      display([image, mask, pred_mask])\n",
        "  else:\n",
        "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
        "    print('after applying mask, uniques of predicted mask:', np.unique(pred_mask), pred_mask.shape)\n",
        "    print('uniques of sample mask', np.unique(sample_mask))\n",
        "    my_iou =  mean_iou(sample_mask, pred_mask)\n",
        "    print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou))\n",
        "\n",
        "    pred_mask = create_mask(pred_mask)\n",
        "    display([sample_image, sample_mask, pred_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "source": [
        "show_predictions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "# Define the Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "source": [
        "LAST_EPOCH=0\n",
        "\n",
        "class ShowPredictions(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1+LAST_EPOCH))\n",
        "\n",
        "class SaveWeights(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    model.save_weights(\"/home/path/to/weights/epoch\"+str(epoch+1+LAST_EPOCH)+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtVCySbXAmL3"
      },
      "source": [
        "# Load a saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCneALDlApZL"
      },
      "source": [
        "model.load_weights(\"/home/path/to/weights/epoch\"+str(LAST_EPOCH)+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DshZEgehOKHV"
      },
      "source": [
        "def build_callbacks():\n",
        "  callbacks = [SaveWeights(),\n",
        "               ShowPredictions()]\n",
        "               \n",
        "  return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dq0PHV__9uT"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugjd4mF7qi8w",
        "scrolled": false
      },
      "source": [
        "STEPS_PER_EPOCH = np.floor(4224/BATCH_SIZE)\n",
        "VALIDATION_STEPS = np.floor(469/BATCH_SIZE)\n",
        "print(STEPS_PER_EPOCH, VALIDATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmGstE4at_DL",
        "scrolled": true
      },
      "source": [
        "model_history = model.fit_generator(my_train_generator,epochs=200,\n",
        "                                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                    validation_data=my_val_generator,\n",
        "                                    validation_steps=VALIDATION_STEPS,\n",
        "                                    callbacks=build_callbacks()\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "source": [
        "loss = model_history.history['mean_iou']\n",
        "val_loss = model_history.history['val_mean_iou']\n",
        "\n",
        "epochs = range(20)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsgTTLU4KnGO"
      },
      "source": [
        "# Save a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGO1w5w2KnGO"
      },
      "source": [
        "tf.keras.models.save_model(model,\"/home/path/to/weights/final.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "# Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVXldSo-0mW"
      },
      "source": [
        "See how the system behaves on a set of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eukWSlYP3ECi"
      },
      "source": [
        "next(my_val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh2wjx3rep89",
        "scrolled": true
      },
      "source": [
        "show_predictions(my_val_generator,num=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}