{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"UNET_4_channels.ipynb","provenance":[{"file_id":"1-HR2Af5kc4ZVm33mHHIpHn7u9yetAIa3","timestamp":1586858671635},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb","timestamp":1584630271203}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uFQ7R6WoflDJ"},"source":["The below UNET code is inspired by this tutorial: https://www.tensorflow.org/tutorials/images/segmentation"]},{"cell_type":"markdown","metadata":{"id":"sJgLTnmvRiwQ"},"source":["# Importing libraries"]},{"cell_type":"code","metadata":{"id":"YQX7R4bhZy5h","executionInfo":{"elapsed":14609,"status":"ok","timestamp":1600416972222,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"1489221f-9c9b-4235-d611-791be57a586d","colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","tf.test.is_gpu_available()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-1-0935608ce417>:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"g87--n2AtyO_"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import numpy as np\n","import random\n","from random import shuffle\n","from PIL import Image\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykea2YRJAQIa"},"source":["# Creating Image Data Generators"]},{"cell_type":"code","metadata":{"id":"Us_GbDYbAjGr"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssoE7TbxAjVk","executionInfo":{"elapsed":2219,"status":"ok","timestamp":1600416972674,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"4fd174cd-15ae-44ef-87f2-f53df0789bd7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"qebASKsngQEr"},"source":["sz = (128, 128)\n","BATCH_SIZE = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZ9HPgoNk9lm"},"source":["# note that for the directories, keras data generators expect a url that points to the folder\n","# that is located at the second level above the images/masks\n","# folder --> directory fed to keras generator\n","#### folder --> sub-directory, usually these stands to the number of clases for the classification model, since it's a segmentation task, it's only one folder\n","####### images or masks\n","\n","# directories of the sugarbeet dataset\n","rgb_images_dir = '/home/path/to/images'\n","annotations_color_dir = '/home/path/to/masks'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1LRYQreLEWK","executionInfo":{"elapsed":76522,"status":"ok","timestamp":1600417053546,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"ded7cc68-5faa-41c9-ced7-a7e80b973822","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# we create two instances with the same arguments\n","data_gen_args = dict(\n","                     horizontal_flip=True,\n","                     vertical_flip=True,\n","                     rescale=1./255,\n","                     validation_split=0.1\n","                     )\n","\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen = ImageDataGenerator(**data_gen_args)\n","\n","#Splitting training and validation without applying same transformations to both datasets:\n","#https://stackoverflow.com/questions/53037510/can-flow-from-directory-get-train-and-validation-data-from-the-same-directory-in\n","#https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","train_image_generator = image_datagen.flow_from_directory(\n","    rgb_images_dir,\n","    target_size = sz,\n","    batch_size=BATCH_SIZE,\n","    class_mode=None,\n","    shuffle=True,\n","    subset='training',\n","    seed=seed)\n","train_mask_generator = mask_datagen.flow_from_directory(\n","    annotations_color_dir,\n","    target_size = sz,\n","    batch_size=BATCH_SIZE,\n","    class_mode=None,\n","    shuffle=True,\n","    subset='training',\n","    seed=seed)\n","\n","val_image_generator = image_datagen.flow_from_directory(\n","    rgb_images_dir,\n","    target_size = sz,\n","    batch_size=BATCH_SIZE,\n","    class_mode=None,\n","    shuffle=True,\n","    subset='validation',\n","    seed=seed)\n","val_mask_generator = mask_datagen.flow_from_directory(\n","    annotations_color_dir,\n","    target_size = sz,\n","    batch_size=BATCH_SIZE,\n","    class_mode=None,\n","    shuffle=True,\n","    subset='validation',\n","    seed=seed)\n","\n"," #about resetting test generators\n","#https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 4224 images belonging to 1 classes.\n","Found 4224 images belonging to 1 classes.\n","Found 469 images belonging to 1 classes.\n","Found 469 images belonging to 1 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AgzTtORV9l_9"},"source":["def normalize(images):\n","  normalized_images = []\n","  for i in range(len(images)):\n","    image = images[i]\n","    max_val = np.ceil(np.max(image))\n","    image = image/max_val\n","    normalized_images.append(image)\n","  return normalized_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NweNGGLRA61m"},"source":["#If this doesn't work, applying mask changes\n","#https://github.com/keras-team/keras-preprocessing/issues/125\n","def prepare_mask(masks):\n","  new_masks = [] \n","  for i in range (np.shape(masks)[0]):\n","    mask_datapoint = masks[i]\n","    r = mask_datapoint[:,:,0]\n","    g = mask_datapoint[:,:,1]\n","    b = mask_datapoint[:,:,2]\n","    r[r<=(150/255)]=0 \n","    r[r>(150/255)]=3\n","    g[g<=(150/255)]=0 \n","    g[g>(150/255)]=1\n","    b[b<=(150/255)]=0 \n","    b[b>(150/255)]=2\n","    x_centroid=[]\n","    y_centroid=[]\n","    for x in range(b.shape[0]):\n","      for y in range(b.shape[1]):\n","        if b[x,y]==2:\n","          x_centroid.append(x)\n","          y_centroid.append(y)\n","    \n","    for i in range(len(x_centroid)):\n","      x=x_centroid[i]\n","      y=y_centroid[i]\n","      augm=1\n","      x_augm_plus=x+augm\n","      x_augm_min=x-augm\n","      y_augm_plus=y+augm\n","      y_augm_min=y-augm\n","      \n","      if(x_augm_plus<b.shape[0]):\n","        b[x_augm_plus,y]=2\n","      if(x_augm_min>0):\n","        b[x_augm_min,y]=2\n","      if(y_augm_plus<b.shape[1]):\n","        b[x,y_augm_plus]=2\n","      if(y_augm_min>0):\n","        b[x,y_augm_min]=2\n","      \n","      if(x_augm_plus>0 and x_augm_plus<b.shape[0] and y_augm_plus>0 and y_augm_plus<b.shape[1]):\n","        b[x_augm_plus,y_augm_plus]=2\n","      if(x_augm_plus>0 and x_augm_plus<b.shape[0] and y_augm_min>0 and y_augm_min<b.shape[1]):\n","        b[x_augm_plus,y_augm_min]=2\n","      if(x_augm_min>0 and x_augm_min<b.shape[0] and y_augm_plus>0 and y_augm_plus<b.shape[1]):\n","        b[x_augm_min,y_augm_plus]=2\n","      if(x_augm_min>0 and x_augm_min<b.shape[0] and y_augm_min>0 and y_augm_min<b.shape[1]):\n","        b[x_augm_min,y_augm_min]=2\n","\n","    merged = np.maximum.reduce([r,g,b])\n","    new_masks.append(merged)\n","  new_masks = np.expand_dims(new_masks,3)\n","  return new_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ6ppr4W-PMC"},"source":["def my_image_mask_generator(image_generator, mask_generator):\n","    train_generator = zip(image_generator, mask_generator)\n","    for (imgs, msks) in train_generator:\n","        msks_total = prepare_mask(msks)\n","        yield (imgs, msks_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdgYj324AqEk"},"source":["def display(display_list):\n","  plt.figure(figsize=(15, 15))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask', 'Fourth image']\n","\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","    plt.axis('off')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b949OT11LoQe"},"source":[" #for joining generators\n"," #https://stackoverflow.com/questions/3211041/how-to-join-two-generators-in-python\n"," #https://stackoverflow.com/questions/49404993/keras-how-to-use-fit-generator-with-multiple-inputs/49405175#comment89680557_49405175\n","my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n","my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZlZvRkitdv9"},"source":["x,y = next(my_train_generator)\n","sample_image = x[2]\n","sample_mask = y[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMOEAR63uxCx"},"source":["z,w = next(my_val_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZSsAHjFfiXE"},"source":["for i in range(len(z)):\n","  sample_image = z[i]\n","  sample_mask = w[i]\n","  display([sample_image,sample_mask])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAOe93FRMk3w"},"source":["# Define the model\n","The model being used here is a modified U-Net. A pretrained model (MobileNetV2) is used as the encoder. The decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). "]},{"cell_type":"code","metadata":{"id":"c6iB4iMvMkX9"},"source":["OUTPUT_CHANNELS = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liCeLH0ctjq7"},"source":["base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n","base_model.summary()\n","# Use the activations of these layers\n","layer_names = [\n","    'block_1_expand_relu',   # 64x64\n","    'block_3_expand_relu',   # 32x32\n","    'block_6_expand_relu',   # 16x16\n","    'block_13_expand_relu',  # 8x8\n","    'block_16_project',      # 4x4\n","]\n","layers = [base_model.get_layer(name).output for name in layer_names]\n","# Create the feature extraction model\n","down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","\n","down_stack.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jzrmK-ok8Hp"},"source":["tf.keras.utils.plot_model(down_stack, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPw8Lzra5_T9"},"source":["The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples."]},{"cell_type":"code","metadata":{"id":"p0ZbfywEbZpJ"},"source":["up_stack_crop = [\n","    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n","    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n","    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n","    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45HByxpVtrPF"},"source":["def unet_model(output_channels, inputs):\n","  #x = inputs\n","\n","  # Downsampling through the model\n","  skips = down_stack(inputs)\n","  print('skips', skips)\n","  last_downsampled_layer = skips[-1]\n","  #print('last_downsampled_layer', last_downsampled_layer)\n","  x = last_downsampled_layer\n","  skips = list(reversed(skips[:-1]))\n","\n","  ##################################################\n","  ############# Crop/Weed branch ###################\n","  ##################################################\n","\n","  # Upsampling and establishing the skip connections\n","  for up, skip in zip(up_stack_crop, skips):\n","    x = up(x)\n","    concat = tf.keras.layers.Concatenate()\n","    x = concat([x, skip])\n","\n","  # This is the last layer of the model\n","  last = tf.keras.layers.Conv2DTranspose(\n","      output_channels, 3, strides=2,\n","      padding='same')  #64x64 -> 128x128\n","  x = last(x)\n","\n","  #classification \n","  return tf.keras.Model(inputs=inputs, outputs=x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tU451nBUclqM","executionInfo":{"elapsed":1020,"status":"ok","timestamp":1600420150518,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"5aaa84bc-524c-435d-e7c3-fa6e4bded724","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n","model = unet_model(OUTPUT_CHANNELS, inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["skips [<tf.Tensor 'model/Identity:0' shape=(None, 64, 64, 96) dtype=float32>, <tf.Tensor 'model/Identity_1:0' shape=(None, 32, 32, 144) dtype=float32>, <tf.Tensor 'model/Identity_2:0' shape=(None, 16, 16, 192) dtype=float32>, <tf.Tensor 'model/Identity_3:0' shape=(None, 8, 8, 576) dtype=float32>, <tf.Tensor 'model/Identity_4:0' shape=(None, 4, 4, 320) dtype=float32>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7mUtS6NUcgzG"},"source":["tf.keras.utils.plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ko6N3Xqf6Zn_"},"source":["# Define the IoU metric"]},{"cell_type":"code","metadata":{"id":"kBRGp8i16fBa"},"source":["def mean_iou(y_true, y_pred):\n","  y_pred = K.cast(create_mask(y_pred), 'float32')\n","  inter = tf.math.count_nonzero(tf.logical_and(tf.not_equal(y_true, 0), tf.equal(y_true,y_pred)))\n","  union = tf.math.count_nonzero(tf.add(y_true, y_pred))\n","  my_iou = tf.cast(inter/union, 'float32')\n","  return my_iou"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0DGH_4T0VYn"},"source":["# Compile the model\n","The network is trying to assign each pixel a label. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class."]},{"cell_type":"code","metadata":{"id":"9zL_5gnVYMiw","executionInfo":{"elapsed":783,"status":"ok","timestamp":1600417870212,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"12f2fbce-20ff-448a-c6e9-85eea4766e1b","colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["model.compile(optimizer=\"adam\",\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[\"accuracy\",mean_iou]\n","              )\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 128, 128, 3) 0                                            \n","__________________________________________________________________________________________________\n","model (Model)                   [(None, 64, 64, 96), 1841984     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","sequential (Sequential)         (None, 8, 8, 512)    1476608     model[1][4]                      \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 8, 8, 1088)   0           sequential[0][0]                 \n","                                                                 model[1][3]                      \n","__________________________________________________________________________________________________\n","sequential_1 (Sequential)       (None, 16, 16, 256)  2507776     concatenate[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 16, 16, 448)  0           sequential_1[0][0]               \n","                                                                 model[1][2]                      \n","__________________________________________________________________________________________________\n","sequential_2 (Sequential)       (None, 32, 32, 128)  516608      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 32, 32, 272)  0           sequential_2[0][0]               \n","                                                                 model[1][1]                      \n","__________________________________________________________________________________________________\n","sequential_3 (Sequential)       (None, 64, 64, 64)   156928      concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 64, 64, 160)  0           sequential_3[0][0]               \n","                                                                 model[1][0]                      \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 128, 128, 4)  5764        concatenate_3[0][0]              \n","==================================================================================================\n","Total params: 6,505,668\n","Trainable params: 4,661,764\n","Non-trainable params: 1,843,904\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D-zkXB11CBCG"},"source":["# Viewing the model performance before training the trainable parameters"]},{"cell_type":"markdown","metadata":{"id":"Tc3MiEO2twLS"},"source":["Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."]},{"cell_type":"code","metadata":{"id":"FEBaO8YKVsl1"},"source":["from tensorflow.keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vJXPz2bUAKg"},"source":["threshold = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UwvIKLZPtxV_"},"source":["def create_mask(pred_mask):\n","  pred_mask = tf.argmax(pred_mask, axis=-1)\n","  pred_mask = pred_mask[..., tf.newaxis]\n","  return pred_mask[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLNsrynNtx4d"},"source":["def show_predictions(dataset=None, num=1):\n","  if dataset:\n","    pred_x, pred_y = next(dataset)\n","    for i in range (num):\n","      image, mask = pred_x[i], pred_y[i]\n","\n","      pred_mask = model.predict(image[tf.newaxis, ...])\n","      my_iou_crop =  mean_iou(mask, pred_mask)\n","      print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n","\n","      pred_mask = create_mask(pred_mask)\n","      print(np.unique(pred_mask), pred_mask.shape)\n","      \n","      display([image, mask, pred_mask])\n","  else:\n","    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n","    print('after applying mask, uniques of predicted mask:', np.unique(pred_mask), pred_mask.shape)\n","    print('uniques of sample mask', np.unique(sample_mask))\n","    my_iou =  mean_iou(sample_mask, pred_mask)\n","    print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou))\n","\n","    pred_mask = create_mask(pred_mask)\n","    display([sample_image, sample_mask, pred_mask])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22AyVYWQdkgk"},"source":["# Define the Callback"]},{"cell_type":"code","metadata":{"id":"wHrHsqijdmL6"},"source":["LAST_EPOCH=312\n","class ShowPredictions(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    show_predictions()\n","    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+LAST_EPOCH+1))\n","\n","class SaveWeights(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    model.save_weights(\"/home/path/to/weights/epoch\"+str(epoch+LAST_EPOCH+1)+\".h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DshZEgehOKHV"},"source":["def build_callbacks():\n","  callbacks = [SaveWeights(),\n","               ShowPredictions()]\n","               \n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtVCySbXAmL3"},"source":["# Load a saved model"]},{"cell_type":"code","metadata":{"id":"HCneALDlApZL"},"source":["model.load_weights(\"/home/path/to/weights/epoch\"+str(LAST_EPOCH)+\".h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_1CC0T4dho3"},"source":["show_predictions()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dq0PHV__9uT"},"source":["# Train the model"]},{"cell_type":"code","metadata":{"id":"Ugjd4mF7qi8w","executionInfo":{"elapsed":722,"status":"ok","timestamp":1600420700966,"user":{"displayName":"Anis Ismail","photoUrl":"","userId":"06455838081248860471"},"user_tz":-180},"outputId":"1ea41d18-a87b-47e2-b7c0-20a9f24370dc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#change these values according to the number of images\n","TOTAL_TRAIN_IMAGES=4224\n","TOTAL_VAL_IMAGES=469\n","\n","STEPS_PER_EPOCH = np.floor(TOTAL_TRAIN_IMAGES/BATCH_SIZE)\n","VALIDATION_STEPS = np.floor(TOTAL_VAL_IMAGES/BATCH_SIZE)\n","print(STEPS_PER_EPOCH, VALIDATION_STEPS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["211.0 23.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rmGstE4at_DL"},"source":["model_history = model.fit_generator(my_train_generator,epochs=100,\n","                                    steps_per_epoch=STEPS_PER_EPOCH,\n","                                    validation_data=my_val_generator,\n","                                    validation_steps=VALIDATION_STEPS,\n","                                    callbacks=build_callbacks()\n","                                    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_mu0SAbt40Q"},"source":["loss = model_history.history['mean_iou']\n","val_loss = model_history.history['val_mean_iou']\n","\n","epochs = range(100)\n","\n","plt.figure()\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([0, 0.2])\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unP3cnxo_N72"},"source":["# Make predictions"]},{"cell_type":"markdown","metadata":{"id":"7BVXldSo-0mW"},"source":["See how the system behaves on a set of images"]},{"cell_type":"code","metadata":{"id":"eukWSlYP3ECi"},"source":["next(my_val_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lh2wjx3rep89"},"source":["show_predictions(my_val_generator,num=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17zs1JqFKEop"},"source":["#Save final model"]},{"cell_type":"code","metadata":{"id":"jXWZ-uRV5xRm"},"source":["tf.keras.models.save_model(model,\"/home//home/path/to/weights/final\"+str(LAST_EPOCH)+\".pb\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlFSg8J1fiYd"},"source":["## Verifying the performance of the model"]},{"cell_type":"code","metadata":{"id":"7nQ5AG6VfiYd"},"source":["from sklearn.metrics import confusion_matrix  \n","import numpy as np\n","\n","def centroid_accuracy(y_true,y_pred):\n"," # ytrue, ypred is a flatten vector\n","  y_true = y_true.flatten()\n","  y_pred=create_mask(y_pred).numpy().flatten()\n","  current = confusion_matrix(y_true, y_pred, labels=[0, 1,2,3])\n","  acc= current[2,2]/np.nansum(current[2])\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKp3kWhOfiYf"},"source":["- Verifying on train data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"jFff3zfpfiYg"},"source":["count=0\n","avg_train_mean_iou=0\n","avg_centroid_acc=0\n","data_size_iou=0\n","data_size_acc=0\n","for i in range(int(STEPS_PER_EPOCH)) :\n","    x,y=next(my_train_generator)\n","    for i in range (x.shape[0]):\n","      \n","      image, mask = x[i], y[i]\n","      pred_mask = model.predict(image[tf.newaxis, ...])\n","      my_iou_crop =  mean_iou(mask, pred_mask).numpy()\n","      centroid_acc=centroid_accuracy(mask, pred_mask)\n","      if not np.isnan(my_iou_crop):\n","          data_size_iou+=1\n","          avg_train_mean_iou+= my_iou_crop\n","      if not np.isnan(centroid_acc):\n","          avg_centroid_acc+=centroid_acc\n","          data_size_acc+=1\n","      if np.isnan(my_iou_crop):\n","          count+=1\n","          print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n","\n","          pred_mask = create_mask(pred_mask)\n","          print(np.unique(pred_mask), pred_mask.shape)\n","\n","          display([image, mask, pred_mask])\n","avg_train_mean_iou/=data_size_iou\n","avg_centroid_acc/=data_size_acc\n","print(\"avg train mean iou:\",avg_train_mean_iou)\n","print(\"pics with nan iou :\",count)\n","print(\"centroid accuracy:\",avg_centroid_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-G-XvugyfiYi"},"source":["- Verifying on validation data"]},{"cell_type":"code","metadata":{"id":"zZWHOQYKfiYj"},"source":["count=0\n","avg_val_mean_iou=0\n","for i in range(int(VALIDATION_STEPS)) :\n","    x,y=next(my_val_generator)\n","    for i in range (x.shape[0]):\n","      image, mask = x[i], y[i]\n","      pred_mask = model.predict(image[tf.newaxis, ...])\n","      my_iou_crop =  mean_iou(mask, pred_mask)\n","      avg_val_mean_iou+= my_iou_crop\n","      if(my_iou_crop<0.1):\n","          count+=1\n","          print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n","\n","          pred_mask = create_mask(pred_mask)\n","          print(np.unique(pred_mask), pred_mask.shape)\n","\n","          display([image, mask, pred_mask])\n","avg_val_mean_iou/=TOTAL_VAL_IMAGES\n","print(\"avg val mean iou:\",avg_val_mean_iou)\n","print(\"pics with iou less than 0.1:\",count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9quZLYjfiYm"},"source":["count=0\n","avg_train_mean_iou=0\n","avg_centroid_acc=0\n","data_size_iou=0\n","data_size_acc=0\n","count2=0\n","for i in range(int(STEPS_PER_EPOCH)) :\n","    x,y=next(my_train_generator)\n","    for i in range (x.shape[0]):\n","      \n","      image, mask = x[i], y[i]\n","      pred_mask = model.predict(image[tf.newaxis, ...])\n","      my_iou_crop =  mean_iou(mask, pred_mask).numpy()\n","      centroid_acc=centroid_accuracy(mask, pred_mask)\n","      print(centroid_acc)\n","      if not np.isnan(my_iou_crop):\n","          data_size_iou+=1\n","          avg_train_mean_iou+= my_iou_crop\n","      if not np.isnan(centroid_acc):\n","          avg_centroid_acc+=centroid_acc\n","          data_size_acc+=1\n","      if np.isnan(my_iou_crop):\n","          count+=1\n","          print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n","\n","          pred_mask = create_mask(pred_mask)\n","          print(np.unique(pred_mask), pred_mask.shape)\n","\n","          display([image, mask, pred_mask])\n","      if my_iou_crop<0.1:\n","          count2+=1\n","          print('My Mean IoU for sample image: ', tf.keras.backend.eval(my_iou_crop))\n","\n","          pred_mask = create_mask(pred_mask)\n","          print(np.unique(pred_mask), pred_mask.shape)\n","\n","          display([image, mask, pred_mask])\n","\n","avg_train_mean_iou/=data_size_iou\n","avg_centroid_acc/=data_size_acc\n","print(\"avg train mean iou:\",avg_train_mean_iou)\n","print(\"pics with iou less than 0.1:\",count2)\n","print(\"pics with nan iou:\",count)\n","print(\"centroid accuracy:\",avg_centroid_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e7Ztt22gfiYp"},"source":["## Adding the 4th channel"]},{"cell_type":"code","metadata":{"id":"RtVtqNS_TUTM"},"source":["predictions = tf.keras.layers.Conv2DTranspose(\n","      4, 3, strides=2,\n","      padding='same')(model.layers[-2].output)\n","model = tf.keras.Model(inputs=inputs, outputs=predictions)\n","model.compile(optimizer=\"adam\",\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[\"accuracy\",mean_iou]\n","              )\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iH-pc9SraT6t"},"source":["weights_list = model.get_weights()\n","weights_list[0][1][0][0]==new_model.get_weights()[0][1][0][0]"],"execution_count":null,"outputs":[]}]}